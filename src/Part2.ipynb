{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM2V6YW+uE5YDj7/ehog3HV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Part 2: Spark Dataframe API**"],"metadata":{"id":"Vt72Tmw_Scso"}},{"cell_type":"markdown","source":["\n","Preliminaries"],"metadata":{"id":"S7b9nFdrScnW"}},{"cell_type":"code","source":["!apt-get update \n","!apt-get install -y openjdk-8-jdk-headless -qq  \n","!apt-get install maven -qq\n","\n","!curl -L \"https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\" > spark-2.4.5-bin-hadoop2.7.tgz\n","!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz \n","!pip install -q findspark \n","\n","!pip install 'apache-airflow==2.2.5'"],"metadata":{"id":"EazXZ0bFTbPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"16g\").getOrCreate()\n","from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString, StringIndexer, StringIndexer, OneHotEncoder, VectorAssembler\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml import Pipeline\n"],"metadata":{"id":"EdwiB9-0ptPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, mean, min, max, col, lit,sum\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import StructType, DoubleType, StringType\n","from airflow import DAG \n","from airflow.operators.python import PythonOperator , BranchPythonOperator\n","from airflow.operators.bash import BashOperator\n","from airflow.utils.task_group import TaskGroup\n","from datetime import datetime\n","from random import randint \n","from os import path"],"metadata":{"id":"PGnbmUm8qxVt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if os.getcwd().split(\"/\")[-1]==\"src\":\n","  %cd ..\n","dir = os.getcwd()\n","!mkdir src \n","!mkdir out \n","%cd ./src "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMU2vHoHad0M","executionInfo":{"status":"ok","timestamp":1650068030113,"user_tz":-120,"elapsed":7,"user":{"displayName":"Xavier Smith","userId":"09565247139698074697"}},"outputId":"534956b9-6290-435a-f109-6e4701add0a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/src\n"]}]},{"cell_type":"markdown","source":["**Task 1**"],"metadata":{"id":"Rxk3h1JYTvic"}},{"cell_type":"markdown","source":["Download the parquet and load to DataFrame"],"metadata":{"id":"bCcphDEIT1sG"}},{"cell_type":"code","source":["!git init \n","!git remote add -f origin https://github.com/databricks/LearningSparkV2 \n","!git config core.sparseCheckout true \n","!echo 'mlflow-project-example/data/*' >> .git/info/sparse-checkout \n","!git pull origin master "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBww219NT7mt","executionInfo":{"status":"ok","timestamp":1650068038238,"user_tz":-120,"elapsed":8130,"user":{"displayName":"Xavier Smith","userId":"09565247139698074697"}},"outputId":"c31c2db9-2fc2-4d45-a817-248a7ee1ff84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized empty Git repository in /content/src/.git/\n","Updating origin\n","remote: Enumerating objects: 1720, done.\u001b[K\n","remote: Counting objects: 100% (143/143), done.\u001b[K\n","remote: Compressing objects: 100% (99/99), done.\u001b[K\n","remote: Total 1720 (delta 40), reused 89 (delta 24), pack-reused 1577\u001b[K\n","Receiving objects: 100% (1720/1720), 76.98 MiB | 13.87 MiB/s, done.\n","Resolving deltas: 100% (527/527), done.\n","From https://github.com/databricks/LearningSparkV2\n"," * [new branch]      master     -> origin/master\n","From https://github.com/databricks/LearningSparkV2\n"," * branch            master     -> FETCH_HEAD\n"]}]},{"cell_type":"code","source":["airbnbDF = spark.read.parquet(dir+\"/src/mlflow-project-example/data/sf-airbnb-clean.parquet\")\n","type(airbnbDF)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ug6gzh_BUEI0","executionInfo":{"status":"ok","timestamp":1650068042801,"user_tz":-120,"elapsed":4565,"user":{"displayName":"Xavier Smith","userId":"09565247139698074697"}},"outputId":"026c8fdf-9946-4cef-da8c-a646377efbb1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["**Task 2**"],"metadata":{"id":"C_3IT1ZuUxWL"}},{"cell_type":"markdown","source":["Lists the minimum price, maximum price, and total row count"],"metadata":{"id":"1UUei0MDWyBI"}},{"cell_type":"code","source":["task2df = airbnbDF.agg(min(\"price\"), max(\"price\")).withColumn(\"total_row\", lit(airbnbDF.count()))\n","task2df.show()\n","try:\n","  task2df.write.csv(dir+\"/out/out_2_2.txt\")\n","except:\n","  print(\"File already created\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AMici442oxYZ","executionInfo":{"status":"ok","timestamp":1650068046175,"user_tz":-120,"elapsed":3377,"user":{"displayName":"Xavier Smith","userId":"09565247139698074697"}},"outputId":"9054a6e9-1d25-4dd8-89fc-4acdb852827f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----------+---------+\n","|min(price)|max(price)|total_row|\n","+----------+----------+---------+\n","|      10.0|   10000.0|     7146|\n","+----------+----------+---------+\n","\n"]}]},{"cell_type":"markdown","source":["**Task 3**"],"metadata":{"id":"zwaqfA85WU8o"}},{"cell_type":"markdown","source":["Calculate the average number of bathrooms and bedrooms"],"metadata":{"id":"zXn93A8IW2xD"}},{"cell_type":"code","source":["task3df = airbnbDF.where(col(\"price\")>5000).where(  (col(\"review_scores_value\")==10 ) | (col(\"review_scores_accuracy\")==10)| (col(\"review_scores_rating\")==10)| (col(\"review_scores_cleanliness\")==10)| (col(\"review_scores_checkin\")==10)| (col(\"review_scores_communication\")==10)| (col(\"review_scores_location\")==10)).agg(mean(\"bedrooms\"), mean(\"bathrooms\"))\n","task3df = task3df.withColumnRenamed(\"avg(bedrooms)\", \"avg_bedrooms\").withColumnRenamed(\"avg(bathrooms)\", \"avg_bathrooms\")\n","task3df.show()\n","try:\n","  task3df.write.csv(dir+\"/out_2_3.txt\")\n","except:\n","  print(\"File already created\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPnJ_GwdpQUg","executionInfo":{"status":"ok","timestamp":1650068046817,"user_tz":-120,"elapsed":645,"user":{"displayName":"Xavier Smith","userId":"09565247139698074697"}},"outputId":"1b8bdf1b-6ab9-43dc-ffcc-25104b4c462b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+-------------+\n","|avg_bedrooms|avg_bathrooms|\n","+------------+-------------+\n","|         3.0|          2.3|\n","+------------+-------------+\n","\n"]}]},{"cell_type":"markdown","source":["**Task 4**"],"metadata":{"id":"XY3XADcaWo1f"}},{"cell_type":"markdown","source":["People can be accomodated by the property with the lowest price and highest rating\n"],"metadata":{"id":"ZuAeAy8RW9Lk"}},{"cell_type":"code","source":["df = airbnbDF.where(col(\"price\")==10000).agg(F.sum(\"beds\")).withColumnRenamed(\"sum(beds)\", \"high\")\n","task4df = airbnbDF.where(col(\"price\")==10).agg(F.sum(\"beds\")).withColumn(\"high\",lit(df.collect()[0][0])).withColumnRenamed(\"sum(beds)\", \"low\")\n","task4df.show()\n","try:\n","  task4df.write.csv(dir+\"/out/out_2_4.txt\")\n","except:\n","  print(\"File already created\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7DX1BzTpa6o","executionInfo":{"status":"ok","timestamp":1650068047991,"user_tz":-120,"elapsed":1175,"user":{"displayName":"Xavier Smith","userId":"09565247139698074697"}},"outputId":"2b14647a-4052-43b6-8913-69d3bb8c709d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----+\n","|low|high|\n","+---+----+\n","|2.0| 5.0|\n","+---+----+\n","\n"]}]},{"cell_type":"markdown","source":["**Task 5**"],"metadata":{"id":"QMxGbZz6Wuma"}},{"cell_type":"markdown","source":["Create an Airflow Dag"],"metadata":{"id":"BMqbrs0cXBRF"}},{"cell_type":"code","source":["%%writefile task_2_5.py\n","## Tasks to be implemented (as there wasnÂ´t any implementation especify, it just pass)\n","def task1():\n","  pass\n","def task2():\n","  pass\n","def task3():\n","  pass\n","def task4():\n","  pass\n","def task5():\n","  pass\n","def task6():\n","  pass\n","# Creation and run of the DAG\n","with DAG(\"my_dag\", start_date =datetime(2022,1,1),schedule_interval=\"@daily\",catchup=False) as dag:\n","  task1 = PythonOperator(task_id=\"task1\", python_callable=task1)\n","\n","  with TaskGroup(\"section_1\", tooltip=\"Tasks for section_1\") as section_1:\n","    task2 = PythonOperator( task_id=\"task2\", python_callable=task2 )\n","    task3 = PythonOperator(task_id=\"task3\",python_callable=task3)\n","  \n","  with TaskGroup(\"section_2\", tooltip=\"Tasks for section_2\") as section_2:\n","      task4 = PythonOperator(task_id=\"task4\", python_callable=task4)\n","      task5 = PythonOperator(task_id=\"task5\",python_callable=task5)\n","      task6 = PythonOperator(task_id=\"task6\", python_callable=task6)\n","  \n","\n","  task1 >> section_1 >> section_2 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6ajWJECoWMc","executionInfo":{"status":"ok","timestamp":1650068047992,"user_tz":-120,"elapsed":6,"user":{"displayName":"Xavier Smith","userId":"09565247139698074697"}},"outputId":"a45b2cd1-08a3-48a6-f3d3-1075f051c07d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing task_2_5.py\n"]}]}]}
